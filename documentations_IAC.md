# Internship Documentation - Dawa Seldon

Welcome to my internship documentation!  
This repository captures everything I do during my internship, including tasks, skills learned, challenges faced, and reflections.

---

## Table of Contents
- [Week 1](12/01/2026-18/01/2026)
- [Week 2](19/01/2026-25/01/2026)
- [Week 3](26/01/2026-01/02/2026)
- [Week 4](02/02/2026-08/02/2026)

---

<details>
<summary><strong>Week 1: [12th to 18th January]</strong></summary>

### Focus of the Week
The primary goal was **conceptual and technical learning** in AI development, focusing on **Natural Language Processing (NLP)**.  
The objective was to explore model architectures, fine-tuning approaches, and their limitations, rather than deploying a final product.

---

### Tasks Completed
- Conducted **sentiment analysis** on movie reviews using DistilBERT:
  - Sentiment classification  
  - Text labeling  
  - Feature extraction  
- Learned that **DistilBERT cannot generate text** due to lack of a decoder.  
- Began work on **Bhutan History chatbot (Yangtshel / BB chatbot)** using:
  - **facebook/opt-1.3b**, capable of natural language generation  
- Worked with **tokenization pipeline** to process raw text into subword tokens.  
- Explored **Parameter-Efficient Fine-Tuning (PEFT)** with **LoRA**:
  - Introduces small trainable adapter layers  
  - Reduces memory and compute requirements  
- Built a basic **Gradio interface** to test chatbot responses.  
- Used **Google Colab** for GPU experimentation, learning session limitations.

---

### Skills Learned / Developed
- Understanding transformer-based models (encoder-only vs decoder).  
- Hands-on experience with **NLP pipelines**:


- Practical knowledge of **fine-tuning limitations** and AI hallucinations.  
- Parameter-Efficient Fine-Tuning (LoRA) implementation.  
- Gradio interface development.  
- Responsible AI development: combining models with external knowledge.

---

### Challenges / Solutions
| Challenge | Solution / Learning |
|-----------|------------------|
| DistilBERT cannot generate text | Transitioned to OPT-1.3B for text generation tasks |
| Limited dataset quality for domain-specific knowledge | Noted importance of high-quality, structured datasets for deep model understanding |
| Potential model hallucinations | Learned that combining models with external knowledge (RAG) reduces inaccuracies |
| Colab session limitations | Adapted experiments to fit session constraints |

---

### Reflection / Notes
- Learned differences in model architectures and task suitability.  
- Understood importance of dataset quality for domain adaptation.  
- Hands-on experience with tokenization, model pipelines, and testing.  
- Realized fine-tuning alone is insufficient; external knowledge is required for educational AI.  
- Next steps: Explore **Retrieval-Augmented Generation (RAG)** for improved factual accuracy.

---

</details>

<details>
<summary><strong>Week 2: [To be added]</strong></summary>
</details>

<details>
<summary><strong>Week 3: [To be added]</strong></summary>
</details>

<details>
<summary><strong>Week 4: [To be added]</strong></summary>
</details>
